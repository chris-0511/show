{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd # 引用套件並縮寫為 pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Read Csv</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = ['./1_政治.csv', './1_科技.csv', './1_運動.csv']\n",
    "train_x_raw, train_y_raw = [], []\n",
    "for train_path in train_paths:\n",
    "    cnt = 0\n",
    "    with open(train_path, newline='',encoding='utf-8') as csvfile:\n",
    "        # 讀取 CSV 檔案內容\n",
    "        rows = csv.reader(csvfile)\n",
    "        # 以迴圈輸出每一列\n",
    "        for row in rows:\n",
    "            cnt+=1\n",
    "            if cnt==5001:\n",
    "                break\n",
    "            else:\n",
    "                train_x_raw.append(row[2])\n",
    "                train_y_raw.append(row[3])\n",
    "\n",
    "test_paths = ['./測試資料.csv']\n",
    "test_x_raw, test_y_raw = [], []\n",
    "for test_path in test_paths:\n",
    "    index = test_paths.index(test_path)\n",
    "    with open(test_path, newline='', encoding='utf-8') as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        for row in rows:\n",
    "            test_x_raw.append(row[2])\n",
    "            test_y_raw.append(row[3])\n",
    "            \n",
    "test_x_raw, test_y_raw = test_x_raw[1:], test_y_raw[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Label to int & Save clean data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data clean file is not exist \n",
      " >> Prepare Data clean file\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "    \"政治\": '0',\n",
    "    \"科技\": '1',\n",
    "    \"運動\": '2'\n",
    "}\n",
    "def to_int(label):\n",
    "    return label_dict[label]\n",
    "\n",
    "train_x_clean_path, train_y_clean_path = \"./data/train_x_clean\", \"./data/train_y_clean\"\n",
    "test_x_clean_path, test_y_clean_path = \"./data/test_x_clean\", \"./data/test_y_clean\"\n",
    "if(not os.path.isfile(\"{0}.npy\".format(train_x_clean_path))):\n",
    "    print(\"Data clean file is not exist \\n >> Prepare Data clean file\")    \n",
    "    train_x_clean, train_y_clean = train_x_raw, [to_int(i) for i in train_y_raw]\n",
    "    test_x_clean, test_y_clean = test_x_raw, [to_int(i) for i in test_y_raw]\n",
    "    \n",
    "    np.save(train_x_clean_path, train_x_clean) ; np.save(train_y_clean_path, train_y_clean) # train\n",
    "    np.save(test_x_clean_path, test_x_clean) ; np.save(test_y_clean_path, test_y_clean) # test\n",
    "else:\n",
    "    print(\"train/test data clean file is exist\")    \n",
    "    print(\">> Loding  \\n   train_x_clean from {0}.npy \\n   train_y_clean from {1}.npy\".format(train_x_clean_path, train_y_clean_path))    \n",
    "    print(\">> Loding  \\n   test_x_clean from {0}.npy \\n   test_y_clean from {1}.npy\".format(test_x_clean_path, test_y_clean_path))    \n",
    "    train_x_clean, train_y_clean = np.load(\"{0}.npy\".format(train_x_clean_path)), np.load(\"{0}.npy\".format(train_y_clean_path))\n",
    "    test_x_clean, test_x_clean = np.load(\"{0}.npy\".format(test_x_clean_path)), np.load(\"{0}.npy\".format(test_y_clean_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Seg Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seg Train/Test data file is not exist\n",
      ">> Prepare process Seg Train/Test data file\n"
     ]
    }
   ],
   "source": [
    "train_x_seg_path, train_y_seg_path = \"./data/train_x_seg\", \"./data/train_y_seg\"\n",
    "test_x_seg_path, test_y_seg_path = \"./data/test_x_seg\", \"./data/test_y_seg\"\n",
    "if(not os.path.isfile(\"{0}.npy\".format(train_x_seg_path))):\n",
    "    print(\"Seg Train/Test data file is not exist\")   \n",
    "    print(\">> Prepare process Seg Train/Test data file\") \n",
    "    train_x_seg, test_x_seg = [' '.join(list(i)) for i in train_x_clean], [' '.join(list(i)) for i in test_x_clean]\n",
    "    train_y_seg, test_y_seg = train_y_clean, test_y_clean\n",
    "    np.save(train_x_seg_path, train_x_seg); np.save(train_y_seg_path, train_y_seg)\n",
    "    np.save(test_x_seg_path, test_x_seg) ; np.save(test_y_seg_path, test_y_seg)\n",
    "else:\n",
    "    print(\"Seg Train/Test data file is exist\")   \n",
    "    print(\">> Loding  \\n   train_x_seg from {0}.npy \\n   train_y_seg from {1}.npy\".format(train_x_seg_path, train_y_seg_path))    \n",
    "    print(\">> Loding  \\n   test_x_seg from {0}.npy \\n   test_y_seg from {1}.npy\".format(test_x_seg_path, test_y_seg_path))    \n",
    "    train_x_seg, train_y_seg = np.load(\"{0}.npy\".format(train_x_seg_path), allow_pickle=True), np.load(\"{0}.npy\".format(train_y_seg_path), allow_pickle=True)\n",
    "    test_x_seg, test_y_seg = np.load(\"{0}.npy\".format(test_x_seg_path), allow_pickle=True), np.load(\"{0}.npy\".format(test_y_seg_path), allow_pickle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    max_sequence_length = 600 # 最長序列長度為n個字\n",
    "    min_word_frequency = 1 # 出現頻率小於n的話 ; 就當成罕見字\n",
    "    \n",
    "    vocab_size = None\n",
    "    category_num = None\n",
    "    \n",
    "    embedding_dim_size = 64 # 詞向量維度\n",
    "    num_filters = 128  # kernal數 #default 256\n",
    "    kernel_size = 5  # kernal尺寸\n",
    "    hidden_dim = 32  # FC神經元數 #default 64\n",
    "    \n",
    "    learning_rate = 0.001 # 學習率\n",
    "    keep_prob = 0.5 \n",
    "    \n",
    "    batch_size = 64 # mini-batch\n",
    "    epoch_size = 30 # epoch\n",
    "    \n",
    "    save_path = './model/cnn_best_validation' # 模型儲存檔名\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test data file is not exist\n",
      ">> Prepare process Train/Val/Test data file\n",
      "Total vocab size: 5260\n",
      ">> Full Train Input Data Shape : (10500, 600) ; Full Train Input Label Shape : (10500, 3)\n",
      ">> Full Val Input Data Shape : (4500, 600) ; Full Val Input Label Shape : (4500, 3)\n",
      ">> Full Test Input Data Shape : (300, 600) ; Full Test Input Label Shape : (300, 3)\n",
      ">> Train Data Shape : (10500, 600) ; Train Label Shape : (10500, 3)\n",
      ">> Val Data Shape : (4500, 600) ; Val Label Shape : (4500, 3)\n",
      ">> Test Data Shape : (300, 600) ; Test Label Shape : (300, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_X_path, train_y_path = \"./data/train_X\", \"./data/train_y\"\n",
    "val_X_path, val_y_path = \"./data/val_X\", \"./data/val_y\"\n",
    "test_X_path, test_y_path = \"./data/test_X\", \"./data/test_y\"\n",
    "if(not os.path.isfile(\"{0}.npy\".format(train_X_path))):\n",
    "    print(\"Train/Val/Test data file is not exist\")   \n",
    "    print(\">> Prepare process Train/Val/Test data file\") \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(config.max_sequence_length, min_frequency=config.min_word_frequency)\n",
    "    train_x_pad = np.array(list(vocab_processor.fit_transform(train_x_seg)))\n",
    "    train_y_pad = tf.keras.utils.to_categorical(train_y_seg)\n",
    "    \n",
    "    test_X = np.array(list(vocab_processor.fit_transform(test_x_seg)))\n",
    "    test_y = tf.keras.utils.to_categorical(test_y_seg)\n",
    "    \n",
    "    config.vocab_size = len(vocab_processor.vocabulary_)\n",
    "    \n",
    "    with open('./data/vocab.txt', 'wt', encoding=\"utf-8\") as w_file:\n",
    "        for vocab in vocab_processor.vocabulary_._reverse_mapping:\n",
    "            w_file.write(vocab + \"\\n\")      \n",
    "    print(\"Total vocab size: {0}\".format(config.vocab_size))\n",
    "    \n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_x_pad, train_y_pad, test_size = 0.3, random_state = 1)\n",
    "    print('>> Full Train Input Data Shape : {0} ; Full Train Input Label Shape : {1}'.format(train_X.shape, train_y.shape))\n",
    "    print('>> Full Val Input Data Shape : {0} ; Full Val Input Label Shape : {1}'.format(val_X.shape, val_y.shape))\n",
    "    print('>> Full Test Input Data Shape : {0} ; Full Test Input Label Shape : {1}'.format(test_X.shape, test_y.shape))\n",
    "    np.save(train_X_path, train_X); np.save(train_y_path, train_y)\n",
    "    np.save(val_X_path, val_X) ; np.save(val_y_path, val_y)\n",
    "    np.save(test_X_path, test_X) ; np.save(test_y_path, test_y)\n",
    "else:\n",
    "    print(\"Train/Val/Test data file is exist\")   \n",
    "    train_X, train_y = np.load(\"{0}.npy\".format(train_X_path)),  np.load(\"{0}.npy\".format(train_y_path))\n",
    "    val_X, val_y = np.load(\"{0}.npy\".format(val_X_path)),  np.load(\"{0}.npy\".format(val_y_path))\n",
    "    test_X, test_y = np.load(\"{0}.npy\".format(test_X_path)),  np.load(\"{0}.npy\".format(test_y_path))\n",
    "    config.vocab_size = sum(1 for line in open(\"./data/vocab.txt\",encoding='utf-8'))\n",
    "\n",
    "config.category_num = train_y.shape[1]\n",
    "print('>> Train Data Shape : {0} ; Train Label Shape : {1}'.format(train_X.shape, train_y.shape))\n",
    "print('>> Val Data Shape : {0} ; Val Label Shape : {1}'.format(val_X.shape, val_y.shape))\n",
    "print('>> Test Data Shape : {0} ; Test Label Shape : {1}'.format(test_X.shape, test_y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class TextCNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # 四個等待輸入的data\n",
    "        self.batch_size = tf.placeholder(tf.int32, [] , name = 'batch_size')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name = 'keep_prob')\n",
    "        \n",
    "        # Initial\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.config.max_sequence_length] , name = 'x')\n",
    "        self.y_label = tf.placeholder(tf.float32, [None, self.config.category_num], name = 'y_label')\n",
    "        \n",
    "        self.cnn()\n",
    "    \n",
    "    def cnn(self):\n",
    "        \"\"\"RNN模型\"\"\"\n",
    "        # 詞向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim_size])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.x)\n",
    "            \n",
    "        #原始single layer\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # single layer\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "        \n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全連接層，後面接dropout以及relu激活\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分類器\n",
    "            self.logits = tf.layers.dense(fc, self.config.category_num, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 損失函數，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_label)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 優化器\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 準確率\n",
    "            correct_pred = tf.equal(tf.argmax(self.y_label, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    1.1, Train Acc:  32.81%, Val Loss:    1.1, Val Acc:  33.78%, Time: 0:00:01 *\n",
      "Iter:     10, Train Loss:   1.08, Train Acc:  43.75%, Val Loss:   1.08, Val Acc:  44.18%, Time: 0:00:03 *\n",
      "Iter:     20, Train Loss:   1.04, Train Acc:  54.69%, Val Loss:   1.03, Val Acc:  59.56%, Time: 0:00:05 *\n",
      "Iter:     30, Train Loss:  0.951, Train Acc:  87.50%, Val Loss:  0.939, Val Acc:  87.47%, Time: 0:00:07 *\n",
      "Iter:     40, Train Loss:  0.773, Train Acc:  90.62%, Val Loss:  0.777, Val Acc:  89.93%, Time: 0:00:09 *\n",
      "Iter:     50, Train Loss:  0.593, Train Acc:  89.06%, Val Loss:  0.549, Val Acc:  90.93%, Time: 0:00:11 *\n",
      "Iter:     60, Train Loss:   0.35, Train Acc:  93.75%, Val Loss:  0.345, Val Acc:  92.69%, Time: 0:00:13 *\n",
      "Iter:     70, Train Loss:  0.274, Train Acc:  93.75%, Val Loss:   0.24, Val Acc:  92.84%, Time: 0:00:15 *\n",
      "Iter:     80, Train Loss:   0.22, Train Acc:  89.06%, Val Loss:  0.189, Val Acc:  93.44%, Time: 0:00:17 *\n",
      "Iter:     90, Train Loss: 0.0894, Train Acc:  98.44%, Val Loss:   0.16, Val Acc:  94.69%, Time: 0:00:19 *\n",
      "Iter:    100, Train Loss:   0.13, Train Acc:  95.31%, Val Loss:  0.149, Val Acc:  94.53%, Time: 0:00:20 \n",
      "Iter:    110, Train Loss: 0.0891, Train Acc:  96.88%, Val Loss:  0.136, Val Acc:  95.42%, Time: 0:00:22 *\n",
      "Iter:    120, Train Loss:  0.118, Train Acc:  96.88%, Val Loss:  0.115, Val Acc:  96.27%, Time: 0:00:24 *\n",
      "Iter:    130, Train Loss:  0.114, Train Acc:  95.31%, Val Loss:  0.107, Val Acc:  96.71%, Time: 0:00:26 *\n",
      "Iter:    140, Train Loss: 0.0586, Train Acc:  98.44%, Val Loss: 0.0981, Val Acc:  96.82%, Time: 0:00:28 *\n",
      "Iter:    150, Train Loss:  0.128, Train Acc:  95.31%, Val Loss: 0.0929, Val Acc:  96.87%, Time: 0:00:30 *\n",
      "Iter:    160, Train Loss: 0.0443, Train Acc:  98.44%, Val Loss:    0.1, Val Acc:  96.44%, Time: 0:00:32 \n",
      "Epoch: 2\n",
      "Iter:    170, Train Loss:  0.126, Train Acc:  96.88%, Val Loss: 0.0871, Val Acc:  97.04%, Time: 0:00:34 *\n",
      "Iter:    180, Train Loss: 0.0494, Train Acc:  98.44%, Val Loss: 0.0802, Val Acc:  97.24%, Time: 0:00:36 *\n",
      "Iter:    190, Train Loss: 0.0926, Train Acc:  96.88%, Val Loss: 0.0749, Val Acc:  97.31%, Time: 0:00:37 *\n",
      "Iter:    200, Train Loss: 0.0144, Train Acc: 100.00%, Val Loss: 0.0719, Val Acc:  97.62%, Time: 0:00:39 *\n",
      "Iter:    210, Train Loss: 0.0289, Train Acc: 100.00%, Val Loss: 0.0677, Val Acc:  97.82%, Time: 0:00:41 *\n",
      "Iter:    220, Train Loss: 0.0313, Train Acc: 100.00%, Val Loss: 0.0655, Val Acc:  97.84%, Time: 0:00:43 *\n",
      "Iter:    230, Train Loss:  0.133, Train Acc:  95.31%, Val Loss: 0.0676, Val Acc:  97.71%, Time: 0:00:45 \n",
      "Iter:    240, Train Loss: 0.0296, Train Acc: 100.00%, Val Loss: 0.0612, Val Acc:  98.00%, Time: 0:00:47 *\n",
      "Iter:    250, Train Loss: 0.0138, Train Acc: 100.00%, Val Loss: 0.0641, Val Acc:  97.82%, Time: 0:00:49 \n",
      "Iter:    260, Train Loss: 0.0421, Train Acc:  98.44%, Val Loss: 0.0663, Val Acc:  97.73%, Time: 0:00:51 \n",
      "Iter:    270, Train Loss: 0.0206, Train Acc: 100.00%, Val Loss: 0.0575, Val Acc:  98.00%, Time: 0:00:53 \n",
      "No optimization for a long time, auto-stopping...\n",
      "訓練完成...\n"
     ]
    }
   ],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"得到已使用時間\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    \n",
    "    return timedelta(seconds = int(round(time_dif)))\n",
    "\n",
    "def feedData(x_batch, y_batch, keep_prob, batch_size, model):\n",
    "    feed_dict = {\n",
    "        model.x: x_batch,\n",
    "        model.y_label: y_batch,\n",
    "        model.keep_prob: keep_prob,\n",
    "        model.batch_size: batch_size\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "best_val_acc = -1.0 # 最佳驗證集準確度\n",
    "last_improved = 0 # 紀錄上一次提升batch \n",
    "require_improvement = 30  # 如果超过n輪未提升，提前结束訓練\n",
    "total_batch = 0  # 總批次\n",
    "print_per_batch = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = TextCNN(config)\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.epoch_size):\n",
    "        print('Epoch: {0}'.format(epoch + 1))\n",
    "        for step in range(0, train_X.shape[0], config.batch_size):\n",
    "            batch_x, batch_y = train_X[step:step + config.batch_size], train_y[step:step + config.batch_size]\n",
    "            \n",
    "            if total_batch % print_per_batch == 0:  \n",
    "                train_loss, train_acc = sess.run([model.loss, model.acc], feed_dict = feedData(batch_x, batch_y, 1.0, batch_x.shape[0], model))\n",
    "                val_loss, val_acc = sess.run([model.loss, model.acc], feed_dict = feedData(val_X, val_y, 1.0, val_X.shape[0], model))\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess = sess, save_path = config.save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "                    \n",
    "                time_dif = get_time_dif(start_time)                               \n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.3}, Train Acc: {2:>7.2%}, Val Loss: {3:>6.3}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, train_loss, train_acc, val_loss, val_acc, time_dif, improved_str))\n",
    "            \n",
    "            # train\n",
    "            sess.run(model.optim, feed_dict = feedData(batch_x, batch_y, 1.0, batch_x.shape[0], model))\n",
    "            total_batch += 1\n",
    "            \n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 驗證集準確度長期不提升，提前结束訓練\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "    print(\"訓練完成...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0xb701239b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0xb701239b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0xb701239b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0xb701239b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb70123fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb70123fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb70123fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb70123fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0xb740e30f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0xb740e30f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0xb740e30f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0xb740e30f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb74101ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb74101ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb74101ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb74101ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn_best_validation\n",
      "Test Loss:   0.25, Test Acc:  93.00%, Time: 0:00:01\n",
      "測試完成...\n",
      ">> Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       100\n",
      "           1       0.88      0.96      0.92       100\n",
      "           2       0.97      0.89      0.93       100\n",
      "\n",
      "    accuracy                           0.93       300\n",
      "   macro avg       0.93      0.93      0.93       300\n",
      "weighted avg       0.93      0.93      0.93       300\n",
      "\n",
      ">> Confusion Matrix...\n",
      "[[94  6  0]\n",
      " [ 1 96  3]\n",
      " [ 4  7 89]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = TextRNN(config)\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess = sess, save_path = config.save_path)  # 讀取保存的模型\n",
    "    \n",
    "    test_loss, test_acc, test_predict_label = sess.run([model.loss, model.acc, model.y_pred_cls], feed_dict = feedData(test_X, test_y, 1.0, test_X.shape[0], model))\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}, Time: {2}'\n",
    "    print(msg.format(test_loss, test_acc, time_dif))\n",
    "    print(\"測試完成...\") \n",
    "    \n",
    "    # 評估\n",
    "    test_label = np.argmax(test_y, 1)\n",
    "    print(\">> Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(test_label, test_predict_label))\n",
    "    \n",
    "    # 混淆矩陣\n",
    "    print(\">> Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(test_label, test_predict_label)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
